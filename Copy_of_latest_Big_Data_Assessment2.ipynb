{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IjeomaM/dotnet-project/blob/main/Copy_of_latest_Big_Data_Assessment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWUdQAQkn_HW",
        "outputId": "499e461d-af75-4964-eafc-ac61fac1fe22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Big Data Assessment\n",
            "sample_data\n",
            "--2024-05-02 10:17:22--  https://downloads.apache.org/spark/spark-3.4.2/spark-3.4.2-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2024-05-02 10:17:23 ERROR 404: Not Found.\n",
            "\n",
            "tar: spark-3.4.2-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 May  2 10:16 ..\n",
            "drwxr-xr-x 1 root root 4096 Apr 30 13:20 sample_data\n",
            "drwxr-xr-x 1 root root 4096 Apr 30 13:20 .\n",
            "drwxr-xr-x 4 root root 4096 Apr 30 13:19 .config\n"
          ]
        }
      ],
      "source": [
        "print(\"Big Data Assessment\")\n",
        "!ls\n",
        "!rm -f spark-3.4.[01]-bin-hadoop3.tgz*\n",
        "!rm -rf spark-3.4.[01]-bin-hadoop3\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\n",
        "!tar -xf spark-3.4.3-bin-hadoop3.tgz\n",
        "!ls -alt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTNj1EzvoJDd",
        "outputId": "162c5827-d569-4988-ee0a-9c2f480a1dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "# install findspark if not already installed\n",
        "!pip3 install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.2-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "# the next line gives us 'local' mode. try 'local[2]' to use 2 cores or 'master:NNNN' to run on Spark standalone cluster at port NNNN\n",
        "spark_conf = SparkConf().setMaster('local[2]').setAppName('MyApp')\n",
        "sc = SparkContext(conf=spark_conf)\n",
        "# see what we have by examining the Spark User Interface\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "80f49ba8-aefa-45a4-ee97-79814e31748a",
        "id": "Rmv-jedr0y6-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Unable to find py4j in /content/spark-3.4.2-bin-hadoop3/python, your SPARK_HOME may not be configured correctly",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-5ee29f8b5e7d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPARK_HOME\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/spark-3.4.2-bin-hadoop3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# the next line gives us 'local' mode. try 'local[2]' to use 2 cores or 'master:NNNN' to run on Spark standalone cluster at port NNNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             raise Exception(\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[1;32m    163\u001b[0m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.4.2-bin-hadoop3/python, your SPARK_HOME may not be configured correctly"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjdETDjrotfN"
      },
      "outputs": [],
      "source": [
        "# TO DO:  install findspark\n",
        "import findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB9zFPpKo15x"
      },
      "outputs": [],
      "source": [
        "# get bike hire file for given year from TfL open data\n",
        "!wget https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2014.zip\n",
        "!unzip cyclehireusagestats-2014.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG-nbeKYpMEs"
      },
      "outputs": [],
      "source": [
        "# read in file\n",
        "\n",
        "!ls\n",
        "file=\"./*csv\"\n",
        "spark = SparkSession.builder.appName(\"bikes\").getOrCreate()\n",
        "Rides_df = (spark.read.format(\"csv\")\n",
        "         .option(\"header\", \"true\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load(file))\n",
        "\n",
        "# show top 10\n",
        "Rides_df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxuiagWDptvY"
      },
      "outputs": [],
      "source": [
        "Rides_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7ogx4KAqInZ"
      },
      "outputs": [],
      "source": [
        "# see how many entries (rows) in data\n",
        "numRows = Rides_df.count()\n",
        "print(\"there are \",numRows,\" rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCa0nK_iqn1S"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import IntegerType, DateType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6vuAyo5qtXK"
      },
      "outputs": [],
      "source": [
        "# Inspect the DataFrame and cast columns to correct data types\n",
        "Rides_df = Rides_df.withColumn(\"Bike Id\", col(\"Bike Id\").cast(IntegerType())) \\\n",
        "           .withColumn(\"Rental Id\", col(\"Rental Id\").cast(IntegerType()))\\\n",
        "           .withColumn(\"Duration\", col(\"Duration\").cast(IntegerType()))\\\n",
        "           .withColumn(\"EndStation Id\", col(\"EndStation Id\").cast(IntegerType()))\\\n",
        "           .withColumn(\"StartStation Id\", col(\"StartStation Id\").cast(IntegerType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iHrIfjCq_DR"
      },
      "outputs": [],
      "source": [
        "Rides_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbdIJjnBrQjs"
      },
      "outputs": [],
      "source": [
        "Ridesnew_df = Rides_df.dropna(subset=[\"End Date\", \"Duration\",\"Start Date\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii4gQgTWr1Am"
      },
      "outputs": [],
      "source": [
        "Ridesnew_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqxSAez_3fS4"
      },
      "outputs": [],
      "source": [
        "# find rows where Duration is zero or less than zero\n",
        "Ridesnew_df.filter(\"Duration <= 0\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm8_kpN5r-VN"
      },
      "outputs": [],
      "source": [
        "Ridesnew1_df = Ridesnew_df.filter((col(\"Duration\") >0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyKNa9CJsBnv"
      },
      "outputs": [],
      "source": [
        "Ridesnew1_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhNS0aNj-hPr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, to_date\n",
        "\n",
        "# Convert the \"Start Date\" column from string to date format and filter out null values\n",
        "Ridesnew1_df = Ridesnew1_df.withColumn(\"Start Date\", to_date(col(\"Start Date\"), \"dd/MM/yyyy HH:mm\")) \\\n",
        "                         .filter(col(\"Start Date\").isNotNull())\n",
        "\n",
        "Ridesnew1_df = Ridesnew1_df.withColumn(\"End Date\", to_date(col(\"End Date\"), \"dd/MM/yyyy HH:mm\")) \\\n",
        "                         .filter(col(\"End Date\").isNotNull())\n",
        "\n",
        "Ridesnew1_df.show(10)\n",
        "Ridesnew1_df.count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI3xFxc-_WuI"
      },
      "outputs": [],
      "source": [
        "Ridesnew1_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUUuGMQM_MYG"
      },
      "outputs": [],
      "source": [
        "#Filter and count where Start Date is not 2014\n",
        "Ridesfiltered_df = Ridesnew1_df.filter((col(\"Start Date\") >= \"2015-01-01\") | (col(\"Start Date\") <= \"2013-12-31\"))\n",
        "Ridesfiltered_df.show(20)\n",
        "Ridesfiltered_df.count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGNduEJu7DKE"
      },
      "outputs": [],
      "source": [
        "RidesUpdated_df = Ridesnew1_df.filter(~((col(\"Start Date\") >= \"2015-01-01\") | (col(\"Start Date\") <= \"2013-12-31\")))\n",
        "RidesUpdated_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H8aV3pT0GP4",
        "outputId": "bfe2e2d7-1f7f-41f3-c533-02f5ceb73b3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicates in RidesFinal_df: 0\n"
          ]
        }
      ],
      "source": [
        "# Count the number of rows before dropping duplicates\n",
        "total_rows = RidesUpdated_df.count()\n",
        "#Count the number of rows after dropping duplicates\n",
        "unique_rows = RidesUpdated_df.dropDuplicates().count()\n",
        "# Calculate the number of duplicates\n",
        "num_duplicates = total_rows - unique_rows\n",
        "print(\"Number of duplicates in RidesFinal_df:\", num_duplicates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL2lddB0EiZr",
        "outputId": "c480e495-dc6d-4935-8a16-1f4ba4636355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "outputs": [
        {
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: Quantile calculation for column Duration with data type StringType is not supported.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-2d94321d5d80>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Remove outliners using IQR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Calculate the first and third quartiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidesUpdated_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Duration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Calculate the interquartile range (IQR)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.3-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mapproxQuantile\u001b[0;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[1;32m   4476\u001b[0m         \u001b[0mrelativeError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4478\u001b[0;31m         \u001b[0mjaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4479\u001b[0m         \u001b[0mjaq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjaq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misStr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.3-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.3-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Quantile calculation for column Duration with data type StringType is not supported."
          ]
        }
      ],
      "source": [
        "# Remove outliners using IQR\n",
        "#Calculate the first and third quartiles\n",
        "q1, q3 = RidesUpdated_df.approxQuantile(\"Duration\", [0.25, 0.75], 0.05)\n",
        "\n",
        "# Calculate the interquartile range (IQR)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Define lower and upper bounds for outlier detection\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "# Filter out rows with values outside the bounds\n",
        "\n",
        "RidesFinal_df = RidesUpdated_df.filter((col(\"Duration\") >= lower_bound) & (col(\"Duration\") <= upper_bound))\n",
        "\n",
        "print(f\"Dataframe without outliers: {RidesFinal_df.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkrHKA9vsA7J"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import Python libs\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# look at values (random sampling)\n",
        "sample_df=RidesFinal_df.sample(0.001) # 0.001 of 0.5M is 500\n",
        "print(\"sample has \",sample_df.count(),\" elements\")\n",
        "plotSample_pdf = sample_df[[\"Bike Id\", \"Duration\"]].toPandas()\n",
        "plotSample_pdf.plot(kind=\"hist\", y=\"Duration\", x=\"Bike Id\") # Hist plot (duration .v. bikeID)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoU-Rg3KsArW"
      },
      "outputs": [],
      "source": [
        "RidesFinal_df.agg({\"Duration\": \"min\"}).show()\n",
        "RidesFinal_df.agg({\"Duration\": \"mean\"}).show()\n",
        "RidesFinal_df.agg({\"Duration\": \"max\"}).show()\n",
        "RidesFinal_df.agg({\"Duration\": \"skewness\"}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgxLMJ8ssAXe"
      },
      "outputs": [],
      "source": [
        "# so vast majority of rides in this month have \"Duration\" under 2 hours\n",
        "total = RidesFinal_df.count()\n",
        "numExceed = RidesFinal_df.filter(\"Duration > 7200\").count()\n",
        "print(\"Of all rides, percentage over 2 hours:\", (numExceed*100)/total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ2lAVYm6KoI"
      },
      "outputs": [],
      "source": [
        "# TO DO: add a histogram of ride Duration less or equal to 2 hours\n",
        "\n",
        "less_equal2_df = RidesFinal_df.filter(\"Duration <= 7200\")\n",
        "\n",
        "plot_pdf = less_equal2_df[[\"Duration\"]].toPandas() # take just \"Duration\" col, convert to pandas dataframe structure ready to plot\n",
        "#print(\"we have \", plot_df.count(),\" elements to plot\")\n",
        "print(\"we use '_pdf' to denote 'pandas data frame\")\n",
        "\n",
        "plot_pdf.plot(kind=\"hist\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8lgMqHl6u-m"
      },
      "outputs": [],
      "source": [
        "# can refine further\n",
        "# e.g. remove rides with Duration of 0 seconds or over 3600\n",
        "rides_df = RidesFinal_df.filter(\"Duration > 0\").filter(\"Duration <= 3600\")[[\"Duration\"]]\n",
        "# now convert to Pandas df and plot\n",
        "rides_df.toPandas().plot(kind=\"hist\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWYoc9oE7CSS"
      },
      "outputs": [],
      "source": [
        "# TO DO: manually use threshold for given month\n",
        "print(\"histogram with 100 bins and log of frequency for rides lasting more than 5 minutes and less than 10mins\")\n",
        "RidesFinal_df.filter(\"Duration > 300\").filter(\"Duration < 600\")[[\"Duration\"]].toPandas().plot(kind=\"hist\", bins=10, log=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RDlANAuLq9W"
      },
      "outputs": [],
      "source": [
        "# group \"StartStation Id\" and sum their \"Duration\", sort in descending order\n",
        "StartStation_df = (RidesFinal_df.select(\"StartStation Id\", \"StartStation Name\", \"EndStation Id\", \"EndStation Name\",\"Duration\")\n",
        "         .groupBy(\"StartStation Id\",\"StartStation Name\")).agg(sum(\"Duration\").alias(\"Total Duration\")).orderBy(\"Total Duration\", ascending=False)\n",
        "# show top 10\n",
        "StartStation_df.show(10)\n",
        "\n",
        "print('Note \"Duration\" is in seconds (see above table)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmvh99HaLtLo"
      },
      "outputs": [],
      "source": [
        " # group \"EndStation Id\" and sum their \"Duration\", sort in descending order\n",
        "EndStation_df = RidesFinal_df.select(\"EndStation Id\", \"EndStation Name\", \"Duration\")\\\n",
        ".groupBy(\"EndStation Id\", \"EndStation Name\").agg(sum(\"Duration\").alias(\"Total Duration\")).orderBy(\"Total Duration\", ascending=False)\n",
        "# show top 10\n",
        "EndStation_df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMDZ71FLuJ-_"
      },
      "outputs": [],
      "source": [
        "#determine %age of rides >3 days and interpret what this means\n",
        "\n",
        "\n",
        "# Count the number of rides with duration greater than 3 days\n",
        "Rides_greater_3_days = RidesFinal_df.filter(col(\"Duration\") > (259200))  # Assuming duration is in minutes, convert 3 days to minutes (3 * 24 * 60)\n",
        "\n",
        "# Calculate the total number of rides\n",
        "Total_rides = RidesFinal_df.count()\n",
        "Rides_greater_3_days.count()\n",
        "\n",
        "# Calculate the percentage of rides greater than 3 days\n",
        "Percentage_greater_3_days = (Rides_greater_3_days.count() / Total_rides) * 100\n",
        "\n",
        "print(\"Percentage of rides greater than 3 days:\", Percentage_greater_3_days)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33O4N0wruQjA"
      },
      "outputs": [],
      "source": [
        "#determine how many rows have Duration of over a day\n",
        "# Calculate the number of rows where the duration is over a day\n",
        "rows_over_day = RidesFinal_df.filter(col(\"Duration\") > (24 * 60))  # Assuming duration is in minutes, convert 1 day to minutes (24 * 60)\n",
        "\n",
        "# Count the number of rows\n",
        "count_over_day = rows_over_day.count()\n",
        "\n",
        "print(\"Number of rows with duration over a day:\", count_over_day)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VdyiRb1tUqR"
      },
      "outputs": [],
      "source": [
        "#Determine popular routes by 'mode' (e.g. short, day hire, commuting etc)\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Define rules for determining the mode based on ride duration\n",
        "mode_description = (\n",
        "    when(col(\"Duration\") <= 30, \"short\")  # Rides with duration less than or equal to 30 minutes are considered \"short\"\n",
        "    .when((col(\"Duration\") > 30) & (col(\"Duration\") <= 1440), \"Day Hire\")  # Rides between 30 minutes and 1 day are considered \"day hire\"\n",
        "    .otherwise(\"Commuting\")  # All other rides are considered \"commuting\"\n",
        ")\n",
        "\n",
        "# Create the new \"Mode\" field based on the defined rules\n",
        "RidesFinal_Mode_df = RidesFinal_df.withColumn(\"Mode\", mode_description)\n",
        "\n",
        "# Filter the dataset based on the mode\n",
        "ShortMode_df = RidesFinal_Mode_df.filter(col(\"Mode\") == \"Short\")\n",
        "DayHireMode_df = RidesFinal_Mode_df.filter(col(\"Mode\") == \"Day Hire\")\n",
        "CommutingMode_df = RidesFinal_Mode_df.filter(col(\"Mode\") == \"Commuting\")\n",
        "\n",
        "RidesFinal_Mode_df.show(10)\n",
        "\n",
        "# Group and aggregate data of the modes tondetermine the popular routes\n",
        "ShortPopularRoutes_df = (ShortMode_df\n",
        "                           .groupBy(\"StartStation Id\", \"EndStation Id\")\n",
        "                           .count()\n",
        "                           .orderBy(\"count\", ascending=False))\n",
        "\n",
        "DayHirePopularRoutes_df = (DayHireMode_df\n",
        "                              .groupBy(\"StartStation Id\", \"EndStation Id\")\n",
        "                              .count()\n",
        "                              .orderBy(\"count\", ascending=False))\n",
        "\n",
        "CommutingPopularRoutes_df = (CommutingMode_df\n",
        "                                .groupBy(\"StartStation Id\", \"EndStation Id\")\n",
        "                                .count()\n",
        "                                .orderBy(\"count\", ascending=False))\n",
        "\n",
        "print(f\"Popular Routes for Short Duration: {ShortPopularRoutes_df.count()}\")\n",
        "print(f\"Popular Routes for a Day Hire Duration: {DayHirePopularRoutes_df.count()}\")\n",
        "print(f\"Popular Routes for Communting Duration: {CommutingPopularRoutes_df.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qttOolg_t77u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEmsPNbo869H"
      },
      "outputs": [],
      "source": [
        "# Filter rows where the \"station name\" column equals \"Baylis Road, Waterloo\"\n",
        "Baylis_df = RidesFinal_df.filter(col(\"StartStation Name\") == \"Baylis Road, Waterloo\")\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "Baylis_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmSSHcKv9vbA"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean duration of Baylis Station\n",
        "mean_duration = Baylis_df.agg({\"Duration\": \"mean\"}).collect()[0][0]\n",
        "\n",
        "print(\"Mean Duration for Baylis Road, Waterloo:\", mean_duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMnvVRoH-Mj1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Filter rows where the \"station name\" column is not \"Baylis Road, Waterloo\"\n",
        "other_stations_df = RidesFinal_df.filter(~(col(\"StartStation Name\") == \"Baylis Road, Waterloo\"))\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "other_stations_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahqol1uZ-n4G"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean duration of other stations\n",
        "mean_duration_other_stations = other_stations_df.agg({\"Duration\": \"mean\"}).collect()[0][0]\n",
        "\n",
        "print(\"Mean Duration for Other Stations:\", mean_duration_other_stations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAsV8FDr-xv1"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.functions import month, to_date\n",
        "\n",
        "# Extract month from the \"Start Date\" column for both DataFrames\n",
        "Baylis_month_df = Baylis_df.withColumn(\"Start Date\", to_date(\"Start Date\", \"dd-MM-yyyy mm:ss\"))\\\n",
        "                     .withColumn(\"Month\",month(\"Start Date\"))\n",
        "other_stations_month_df = other_stations_df.withColumn(\"Start Date\", to_date(\"Start Date\", \"dd-MM-yyyy mm:ss\"))\\\n",
        "                     .withColumn(\"Month\",month(\"Start Date\"))\n",
        "\n",
        "# Show the resulting DataFrames\n",
        "Baylis_month_df.show()\n",
        "other_stations_month_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPlspP992Ozs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1HhrlkPJWRH"
      },
      "outputs": [],
      "source": [
        "#from pyspark.sql.functions import month, col, to_date\n",
        "# Extract month from the \"Start Date\" column for both DataFrames\n",
        "Baylis_df = Baylis_df.withColumn(\"Month\", month(\"Start Date\"))\n",
        "Other_stations_df = other_stations_df.withColumn(\"Month\", month(\"Start Date\"))\n",
        "\n",
        "# Group by month and StartStation Name for both DataFrames\n",
        "Baylis_Monthgrouped_df = Baylis_df.groupBy(\"Month\", \"StartStation Name\").count()\n",
        "Other_stations_Monthgrouped_df = Other_stations_df.groupBy(\"Month\", \"StartStation Name\").count()\n",
        "\n",
        "# Show the resulting DataFrames\n",
        "Baylis_Monthgrouped_df.show()\n",
        "Other_stations_Monthgrouped_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHXcF_vYQCVK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4SL-9ZMQDd0"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "# Group by month and calculate the mean duration for Baylis station\n",
        "Baylis_mean_duration_df = Baylis_df.groupBy(\"Month\").agg(mean(\"Duration\").alias(\"Baylis Mean Duration\"))\n",
        "\n",
        "# Group by month and calculate the mean duration for other stations\n",
        "Other_stations_mean_duration_df = Other_stations_df.groupBy(\"Month\").agg(mean(\"Duration\").alias(\"Other Stations Mean Duration\"))\n",
        "\n",
        "# Show the resulting DataFrames\n",
        "Baylis_mean_duration_df.show()\n",
        "Other_stations_mean_duration_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kASxTX-aMQLs"
      },
      "outputs": [],
      "source": [
        "# Join the two DataFrames on the \"Month\" column\n",
        "Baylis_Others_df = Baylis_mean_duration_df.join(Other_stations_mean_duration_df, \"Month\", \"outer\")\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "Baylis_Others_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7R_HDGNNYdu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract data from the joined DataFrame\n",
        "months = Baylis_Others_df.select(\"Month\").collect()\n",
        "baylis_mean_duration = Baylis_Others_df.select(\"Baylis Mean Duration\").collect()\n",
        "other_stations_mean_duration = Baylis_Others_df.select(\"Other Stations Mean Duration\").collect()\n",
        "\n",
        "# Convert the data to lists for plotting\n",
        "months = [row[\"Month\"] for row in months]\n",
        "baylis_mean_duration = [row[\"Baylis Mean Duration\"] for row in baylis_mean_duration]\n",
        "other_stations_mean_duration = [row[\"Other Stations Mean Duration\"] for row in other_stations_mean_duration]\n",
        "\n",
        "# Plot the mean duration for Baylis station\n",
        "plt.plot(months, baylis_mean_duration, label=\"Baylis Road, Waterloo\")\n",
        "\n",
        "# Plot the mean duration for other stations\n",
        "plt.plot(months, other_stations_mean_duration, label=\"Other Stations\")\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Mean Duration\")\n",
        "plt.title(\"Mean Duration Comparison\")\n",
        "plt.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqCpWMbD1aDz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Group by EndStation Name and count the frequency\n",
        "station_frequency = RidesFinal_df.groupBy(\"EndStation Name\").agg(count(\"*\").alias(\"Frequency\")).orderBy(\"Frequency\", ascending=False)\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "station_frequency.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gWOo2zv5FC_"
      },
      "outputs": [],
      "source": [
        "# Plot routes from Baylis to other stations(heatmap for popularity)\n",
        "\n",
        "# Filter rows where the StartStation Name is Baylis\n",
        "baylis_df = RidesFinal_df.filter(col(\"StartStation Name\") == \"Baylis Road, Waterloo\")\n",
        "\n",
        "# Filter the top ten stations\n",
        "top_ten_stations = station_frequency.limit(20)\n",
        "\n",
        "# Join the filtered Baylis DataFrame with the station_frequency DataFrame\n",
        "baylis_with_frequency_df = baylis_df.join(top_ten_stations, \"EndStation Name\", \"inner\")\n",
        "\n",
        "# Aggregate data to count frequency of rides from Baylis to each top station\n",
        "baylis_to_top_stations_freq = baylis_with_frequency_df.groupBy(\"StartStation Name\", \"EndStation Name\").count()\n",
        "\n",
        "# Convert DataFrame to a format suitable for plotting heatmap\n",
        "heatmap_data = baylis_to_top_stations_freq.toPandas()\n",
        "\n",
        "# Plot heatmap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "heatmap = sns.heatmap(heatmap_data.pivot(index=\"StartStation Name\", columns=\"EndStation Name\", values=\"count\"), cmap=\"YlGnBu\", annot=True, fmt=\"g\")\n",
        "heatmap.set_title(\"Popularity Heatmap: Baylis Station to Top 10 Stations\")\n",
        "plt.xlabel(\"End Station\")\n",
        "plt.ylabel(\"Start Station (Baylis)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX9J_7NyXKXR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Convert joined DataFrame to Pandas DataFrame\n",
        "Baylis_Others_df_pandas = Baylis_Others_df.toPandas()\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t_stat, p_value = ttest_ind(Baylis_Others_df_pandas[\"Baylis Mean Duration\"],\n",
        "                            Baylis_Others_df_pandas[\"Other Stations Mean Duration\"],\n",
        "                            alternative='two-sided')\n",
        "\n",
        "# Set significance level\n",
        "alpha = 0.05\n",
        "\n",
        "\n",
        "\n",
        "print(f\"P_value: {p_value}\")\n",
        "\n",
        "# Interpret results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject Null Hypothesis: There is evidence to suggest that the mean duration of rides from Baylis Road is shorter than other stations.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: There is no evidence to suggest a significant difference in mean ride durations between Baylis Road and other stations.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNp40ehQQEJQmn50r131INX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}