{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IjeomaM/dotnet-project/blob/main/latest_Big_Data_Assessment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWUdQAQkn_HW",
        "outputId": "685d28b2-d3e5-4246-da00-6649033c870f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Big Data Assessment\n",
            "sample_data\n",
            "--2024-05-02 10:22:00--  https://downloads.apache.org/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f8:10a:39da::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 388930980 (371M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.4.3-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.4.3-bin-had 100%[===================>] 370.91M  20.1MB/s    in 20s     \n",
            "\n",
            "2024-05-02 10:22:20 (19.0 MB/s) - ‘spark-3.4.3-bin-hadoop3.tgz’ saved [388930980/388930980]\n",
            "\n",
            "total 379836\n",
            "drwxr-xr-x  1 root root      4096 May  2 10:22 .\n",
            "drwxr-xr-x  1 root root      4096 May  2 10:16 ..\n",
            "drwxr-xr-x  1 root root      4096 Apr 30 13:20 sample_data\n",
            "drwxr-xr-x  4 root root      4096 Apr 30 13:19 .config\n",
            "-rw-r--r--  1 root root 388930980 Apr 15 01:30 spark-3.4.3-bin-hadoop3.tgz\n",
            "drwxr-xr-x 13 1000 1000      4096 Apr 15 01:28 spark-3.4.3-bin-hadoop3\n"
          ]
        }
      ],
      "source": [
        "print(\"Big Data Assessment\")\n",
        "!ls\n",
        "!rm -f spark-3.4.[01]-bin-hadoop3.tgz*\n",
        "!rm -rf spark-3.4.[01]-bin-hadoop3\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz\n",
        "!tar -xf spark-3.4.3-bin-hadoop3.tgz\n",
        "!ls -alt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTNj1EzvoJDd",
        "outputId": "f9d8e393-286d-4fe0-af3a-2e34d0bad425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "# install findspark if not already installed\n",
        "!pip3 install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.3-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkConf, SparkContext\n",
        "# the next line gives us 'local' mode. try 'local[2]' to use 2 cores or 'master:NNNN' to run on Spark standalone cluster at port NNNN\n",
        "spark_conf = SparkConf().setMaster('local[2]').setAppName('MyApp')\n",
        "sc = SparkContext(conf=spark_conf)\n",
        "# see what we have by examining the Spark User Interface\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "Rmv-jedr0y6-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jjdETDjrotfN"
      },
      "outputs": [],
      "source": [
        "# TO DO:  install findspark\n",
        "import findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WB9zFPpKo15x",
        "outputId": "e047ad6b-5fa5-4b13-ee47-a6b6e64eb271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-02 10:23:37--  https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2014.zip\n",
            "Resolving cycling.data.tfl.gov.uk (cycling.data.tfl.gov.uk)... 104.16.98.104, 104.16.97.104\n",
            "Connecting to cycling.data.tfl.gov.uk (cycling.data.tfl.gov.uk)|104.16.98.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 225215129 (215M) [application/zip]\n",
            "Saving to: ‘cyclehireusagestats-2014.zip’\n",
            "\n",
            "cyclehireusagestats 100%[===================>] 214.78M   105MB/s    in 2.1s    \n",
            "\n",
            "2024-05-02 10:23:39 (105 MB/s) - ‘cyclehireusagestats-2014.zip’ saved [225215129/225215129]\n",
            "\n",
            "Archive:  cyclehireusagestats-2014.zip\n",
            "  inflating: 1. Journey Data Extract 05Jan14-02Feb14.csv  \n",
            "  inflating: 10a. Journey Data Extract 14Sep14-27Sep14.csv  \n",
            "  inflating: 10b. Journey Data Extract 28Sep14-11Oct14.csv  \n",
            "  inflating: 11a. Journey Data Extract 12Oct14-08Nov14.csv  \n",
            "  inflating: 11b. Journey Data Extract 12Oct14-08Nov14.csv  \n",
            "  inflating: 12a. Journey Data Extract 09Nov14-06Dec14.csv  \n",
            "  inflating: 12b. Journey Data Extract 09Nov14-06Dec14.csv  \n",
            "  inflating: 13a. Journey Data Extract 07Dec14-21Dec14.csv  \n",
            "  inflating: 13b. Journey Data Extract 22Dec14-03Jan15.csv  \n",
            "  inflating: 14. Journey Data Extract 08Dec13-04Jan14.csv  \n",
            "  inflating: 2. Journey Data Extract 03Feb14-01Mar14.csv  \n",
            "  inflating: 3. Journey Data Extract 02Mar14-31Mar14.csv  \n",
            "  inflating: 4. Journey Data Extract 01Apr14-26Apr14.csv  \n",
            "  inflating: 5. Journey Data Extract 27Apr14-24May14.csv  \n",
            "  inflating: 6. Journey Data Extract 25May14-21Jun14.csv  \n",
            "  inflating: 7. Journey Data Extract 22Jun14-19Jul14.csv  \n",
            "  inflating: 8a Journey Data Extract 20Jul14-31Jul14.csv  \n",
            "  inflating: 8b Journey Data Extract 01Aug14-16Aug14.csv  \n",
            "  inflating: 9a Journey Data Extract 17Aug14-31Aug14.csv  \n",
            "  inflating: 9b Journey Data Extract 01Sep14-13Sep14.csv  \n"
          ]
        }
      ],
      "source": [
        "# get bike hire file for given year from TfL open data\n",
        "!wget https://cycling.data.tfl.gov.uk/usage-stats/cyclehireusagestats-2014.zip\n",
        "!unzip cyclehireusagestats-2014.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aG-nbeKYpMEs",
        "outputId": "2d46f1c6-dd50-4829-cf8a-cb4eade8f630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'10a. Journey Data Extract 14Sep14-27Sep14.csv'  '4. Journey Data Extract 01Apr14-26Apr14.csv'\n",
            "'10b. Journey Data Extract 28Sep14-11Oct14.csv'  '5. Journey Data Extract 27Apr14-24May14.csv'\n",
            "'11a. Journey Data Extract 12Oct14-08Nov14.csv'  '6. Journey Data Extract 25May14-21Jun14.csv'\n",
            "'11b. Journey Data Extract 12Oct14-08Nov14.csv'  '7. Journey Data Extract 22Jun14-19Jul14.csv'\n",
            "'12a. Journey Data Extract 09Nov14-06Dec14.csv'  '8a Journey Data Extract 20Jul14-31Jul14.csv'\n",
            "'12b. Journey Data Extract 09Nov14-06Dec14.csv'  '8b Journey Data Extract 01Aug14-16Aug14.csv'\n",
            "'13a. Journey Data Extract 07Dec14-21Dec14.csv'  '9a Journey Data Extract 17Aug14-31Aug14.csv'\n",
            "'13b. Journey Data Extract 22Dec14-03Jan15.csv'  '9b Journey Data Extract 01Sep14-13Sep14.csv'\n",
            "'14. Journey Data Extract 08Dec13-04Jan14.csv'\t  cyclehireusagestats-2014.zip\n",
            "'1. Journey Data Extract 05Jan14-02Feb14.csv'\t  sample_data\n",
            "'2. Journey Data Extract 03Feb14-01Mar14.csv'\t  spark-3.4.3-bin-hadoop3\n",
            "'3. Journey Data Extract 02Mar14-31Mar14.csv'\t  spark-3.4.3-bin-hadoop3.tgz\n",
            "+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n",
            "|Rental Id|Duration|Bike Id|        End Date|EndStation Id|     EndStation Name|      Start Date|StartStation Id|   StartStation Name|\n",
            "+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n",
            "| 34263367|    1080|   9076|24/06/2014 00:57|          695|Islington Green, ...|24/06/2014 00:39|            311|Foley Street, Fit...|\n",
            "| 34603487|     660|   6328|03/07/2014 11:51|          695|Islington Green, ...|03/07/2014 11:40|             22|Northington Stree...|\n",
            "| 34689078|     120|   2006|05/07/2014 15:09|          357|Howland Street, F...|05/07/2014 15:07|            311|Foley Street, Fit...|\n",
            "| 34724273|    1260|   7904|06/07/2014 16:35|          695|Islington Green, ...|06/07/2014 16:14|            311|Foley Street, Fit...|\n",
            "| 34956750|    1500|   8251|13/07/2014 00:41|           93|Cloudesley Road, ...|13/07/2014 00:16|            311|Foley Street, Fit...|\n",
            "| 34946549|    1380|   6997|12/07/2014 18:35|          430|South Parade, Che...|12/07/2014 18:12|            514|Portman Square, M...|\n",
            "| 35011368|    2820|   4193|14/07/2014 15:49|          573|Limerston Street,...|14/07/2014 15:02|             14|Belgrove Street ,...|\n",
            "| 35027649|     180|   9781|14/07/2014 19:50|          430|South Parade, Che...|14/07/2014 19:47|            216|Old Brompton Road...|\n",
            "| 35057973|     420|   5133|15/07/2014 17:17|          356|South Kensington ...|15/07/2014 17:10|            216|Old Brompton Road...|\n",
            "| 35086123|    1200|    190|16/07/2014 09:31|          290|Winsland Street, ...|16/07/2014 09:11|            430|South Parade, Che...|\n",
            "+---------+--------+-------+----------------+-------------+--------------------+----------------+---------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# read in file\n",
        "\n",
        "!ls\n",
        "file=\"./*csv\"\n",
        "spark = SparkSession.builder.appName(\"bikes\").getOrCreate()\n",
        "Rides_df = (spark.read.format(\"csv\")\n",
        "         .option(\"header\", \"true\")\n",
        "         .option(\"inferSchema\", \"true\")\n",
        "         .load(file))\n",
        "\n",
        "# show top 10\n",
        "Rides_df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XxuiagWDptvY",
        "outputId": "a9e65514-d803-4b72-8045-8047b44a7348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Rental Id: string (nullable = true)\n",
            " |-- Duration: string (nullable = true)\n",
            " |-- Bike Id: string (nullable = true)\n",
            " |-- End Date: string (nullable = true)\n",
            " |-- EndStation Id: string (nullable = true)\n",
            " |-- EndStation Name: string (nullable = true)\n",
            " |-- Start Date: string (nullable = true)\n",
            " |-- StartStation Id: string (nullable = true)\n",
            " |-- StartStation Name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Rides_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g7ogx4KAqInZ",
        "outputId": "bdf06cda-557a-44d6-afd4-7b0664551460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are  11481596  rows\n"
          ]
        }
      ],
      "source": [
        "# see how many entries (rows) in data\n",
        "numRows = Rides_df.count()\n",
        "print(\"there are \",numRows,\" rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kCa0nK_iqn1S"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import IntegerType, DateType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6vuAyo5qtXK"
      },
      "outputs": [],
      "source": [
        "# Inspect the DataFrame and cast columns to correct data types\n",
        "Rides_df = Rides_df.withColumn(\"Bike Id\", col(\"Bike Id\").cast(IntegerType())) \\\n",
        "           .withColumn(\"Rental Id\", col(\"Rental Id\").cast(IntegerType()))\\\n",
        "           .withColumn(\"Duration\", col(\"Duration\").cast(IntegerType()))\\\n",
        "           .withColumn(\"EndStation Id\", col(\"EndStation Id\").cast(IntegerType()))\\\n",
        "           .withColumn(\"StartStation Id\", col(\"StartStation Id\").cast(IntegerType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iHrIfjCq_DR"
      },
      "outputs": [],
      "source": [
        "Rides_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SbdIJjnBrQjs"
      },
      "outputs": [],
      "source": [
        "Ridesnew_df = Rides_df.dropna(subset=[\"End Date\", \"Duration\",\"Start Date\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ii4gQgTWr1Am",
        "outputId": "3a1a1b8a-63f4-4939-db8a-c5455e6862d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10242483"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "Ridesnew_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zqxSAez_3fS4",
        "outputId": "19f51f6c-da0a-4505-dfcf-0446883e2313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32502"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# find rows where Duration is zero or less than zero\n",
        "Ridesnew_df.filter(\"Duration <= 0\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Sm8_kpN5r-VN"
      },
      "outputs": [],
      "source": [
        "Ridesnew1_df = Ridesnew_df.filter((col(\"Duration\") >0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wyKNa9CJsBnv",
        "outputId": "f1d3af4d-a923-4c25-8040-c7eb2eb7abb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10209981"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "Ridesnew1_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OhNS0aNj-hPr",
        "outputId": "ca12abe3-f0c7-470b-e04f-445cd0ef4c2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------+----------+-------------+--------------------+----------+---------------+--------------------+\n",
            "|Rental Id|Duration|Bike Id|  End Date|EndStation Id|     EndStation Name|Start Date|StartStation Id|   StartStation Name|\n",
            "+---------+--------+-------+----------+-------------+--------------------+----------+---------------+--------------------+\n",
            "| 34263367|    1080|   9076|2014-06-24|          695|Islington Green, ...|2014-06-24|            311|Foley Street, Fit...|\n",
            "| 34603487|     660|   6328|2014-07-03|          695|Islington Green, ...|2014-07-03|             22|Northington Stree...|\n",
            "| 34689078|     120|   2006|2014-07-05|          357|Howland Street, F...|2014-07-05|            311|Foley Street, Fit...|\n",
            "| 34724273|    1260|   7904|2014-07-06|          695|Islington Green, ...|2014-07-06|            311|Foley Street, Fit...|\n",
            "| 34956750|    1500|   8251|2014-07-13|           93|Cloudesley Road, ...|2014-07-13|            311|Foley Street, Fit...|\n",
            "| 34946549|    1380|   6997|2014-07-12|          430|South Parade, Che...|2014-07-12|            514|Portman Square, M...|\n",
            "| 35011368|    2820|   4193|2014-07-14|          573|Limerston Street,...|2014-07-14|             14|Belgrove Street ,...|\n",
            "| 35027649|     180|   9781|2014-07-14|          430|South Parade, Che...|2014-07-14|            216|Old Brompton Road...|\n",
            "| 35057973|     420|   5133|2014-07-15|          356|South Kensington ...|2014-07-15|            216|Old Brompton Road...|\n",
            "| 35086123|    1200|    190|2014-07-16|          290|Winsland Street, ...|2014-07-16|            430|South Parade, Che...|\n",
            "+---------+--------+-------+----------+-------------+--------------------+----------+---------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10209981"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, to_date\n",
        "\n",
        "# Convert the \"Start Date\" column from string to date format and filter out null values\n",
        "Ridesnew1_df = Ridesnew1_df.withColumn(\"Start Date\", to_date(col(\"Start Date\"), \"dd/MM/yyyy HH:mm\")) \\\n",
        "                         .filter(col(\"Start Date\").isNotNull())\n",
        "\n",
        "Ridesnew1_df = Ridesnew1_df.withColumn(\"End Date\", to_date(col(\"End Date\"), \"dd/MM/yyyy HH:mm\")) \\\n",
        "                         .filter(col(\"End Date\").isNotNull())\n",
        "\n",
        "Ridesnew1_df.show(10)\n",
        "Ridesnew1_df.count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "mI3xFxc-_WuI",
        "outputId": "ed4ee55e-ee9b-43bc-a93a-61671309d22f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Rental Id: string (nullable = true)\n",
            " |-- Duration: string (nullable = true)\n",
            " |-- Bike Id: string (nullable = true)\n",
            " |-- End Date: date (nullable = true)\n",
            " |-- EndStation Id: string (nullable = true)\n",
            " |-- EndStation Name: string (nullable = true)\n",
            " |-- Start Date: date (nullable = true)\n",
            " |-- StartStation Id: string (nullable = true)\n",
            " |-- StartStation Name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Ridesnew1_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VUUuGMQM_MYG",
        "outputId": "487ee901-8155-4e06-9d38-51d1ef8897f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------+----------+-------------+--------------------+----------+---------------+--------------------+\n",
            "|Rental Id|Duration|Bike Id|  End Date|EndStation Id|     EndStation Name|Start Date|StartStation Id|   StartStation Name|\n",
            "+---------+--------+-------+----------+-------------+--------------------+----------+---------------+--------------------+\n",
            "| 29440493|    1380|   1056|2013-12-12|          695|Islington Green, ...|2013-12-12|            311|Foley Street, Fit...|\n",
            "| 29458775|    1680|   8123|2013-12-13|          695|Islington Green, ...|2013-12-13|            311|Foley Street, Fit...|\n",
            "| 29533719|    1200|   6986|2013-12-18|          695|Islington Green, ...|2013-12-18|            311|Foley Street, Fit...|\n",
            "| 29552829|    1320|   9054|2013-12-19|          695|Islington Green, ...|2013-12-19|            311|Foley Street, Fit...|\n",
            "| 29660647|     180|   6282|2013-12-28|          695|Islington Green, ...|2013-12-28|            234|Liverpool Road (N...|\n",
            "| 29575688|     480|  10781|2013-12-21|          590|Greenberry Street...|2013-12-21|            242|Beaumont Street, ...|\n",
            "| 29534642|     300|   6276|2013-12-19|          121|Baker Street, Mar...|2013-12-19|            590|Greenberry Street...|\n",
            "| 29498972|     420|   2040|2013-12-17|          121|Baker Street, Mar...|2013-12-17|            590|Greenberry Street...|\n",
            "| 29372890|     480|   4285|2013-12-09|          590|Greenberry Street...|2013-12-09|            242|Beaumont Street, ...|\n",
            "| 29399761|     420|   7365|2013-12-11|          121|Baker Street, Mar...|2013-12-11|            590|Greenberry Street...|\n",
            "| 29408086|     960|   6642|2013-12-11|          216|Old Brompton Road...|2013-12-11|            303|Albert Gate, Hyde...|\n",
            "| 29462236|     900|  11433|2013-12-14|          607|Putney Bridge Sta...|2013-12-14|            607|Putney Bridge Sta...|\n",
            "| 29524323|     300|  11191|2013-12-18|          432|Exhibition Road M...|2013-12-18|            292|Montpelier Street...|\n",
            "| 29602015|     300|   4407|2013-12-24|          743| Oxford Road, Putney|2013-12-24|            743| Oxford Road, Putney|\n",
            "| 29519648|     720|   6896|2013-12-18|          272|Baylis Road, Wate...|2013-12-18|            199|Great Tower Stree...|\n",
            "| 29502792|     720|   8916|2013-12-17|          272|Baylis Road, Wate...|2013-12-17|            199|Great Tower Stree...|\n",
            "| 29505185|     300|    795|2013-12-17|          574|Eagle Wharf Road,...|2013-12-17|             73|Old Street Statio...|\n",
            "| 29512299|     720|   7473|2013-12-17|          199|Great Tower Stree...|2013-12-17|            272|Baylis Road, Wate...|\n",
            "| 29569564|     780|  11500|2013-12-20|          199|Great Tower Stree...|2013-12-20|            272|Baylis Road, Wate...|\n",
            "| 29557540|     660|    821|2013-12-20|          272|Baylis Road, Wate...|2013-12-20|            199|Great Tower Stree...|\n",
            "+---------+--------+-------+----------+-------------+--------------------+----------+---------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "345690"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "#Filter and count where Start Date is not 2014\n",
        "Ridesfiltered_df = Ridesnew1_df.filter((col(\"Start Date\") >= \"2015-01-01\") | (col(\"Start Date\") <= \"2013-12-31\"))\n",
        "Ridesfiltered_df.show(20)\n",
        "Ridesfiltered_df.count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eGNduEJu7DKE",
        "outputId": "42b85395-b69f-4bf9-bca1-8c5d14d2a28a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9864291"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "RidesUpdated_df = Ridesnew1_df.filter(~((col(\"Start Date\") >= \"2015-01-01\") | (col(\"Start Date\") <= \"2013-12-31\")))\n",
        "RidesUpdated_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9H8aV3pT0GP4",
        "outputId": "d2cc36bc-9206-4c7e-bc60-e31539ad502f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of duplicates in RidesFinal_df: 0\n"
          ]
        }
      ],
      "source": [
        "# Count the number of rows before dropping duplicates\n",
        "total_rows = RidesUpdated_df.count()\n",
        "#Count the number of rows after dropping duplicates\n",
        "unique_rows = RidesUpdated_df.dropDuplicates().count()\n",
        "# Calculate the number of duplicates\n",
        "num_duplicates = total_rows - unique_rows\n",
        "print(\"Number of duplicates in RidesFinal_df:\", num_duplicates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UL2lddB0EiZr",
        "outputId": "f85330c7-6177-4d49-ea73-6363e408594a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: Quantile calculation for column Duration with data type StringType is not supported.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-2d94321d5d80>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Remove outliners using IQR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Calculate the first and third quartiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidesUpdated_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Duration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Calculate the interquartile range (IQR)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.3-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mapproxQuantile\u001b[0;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[1;32m   4476\u001b[0m         \u001b[0mrelativeError\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4478\u001b[0;31m         \u001b[0mjaq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelativeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4479\u001b[0m         \u001b[0mjaq_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjaq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4480\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misStr\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mjaq_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.3-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.3-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Quantile calculation for column Duration with data type StringType is not supported."
          ]
        }
      ],
      "source": [
        "# Remove outliners using IQR\n",
        "#Calculate the first and third quartiles\n",
        "q1, q3 = RidesUpdated_df.approxQuantile(\"Duration\", [0.25, 0.75], 0.05)\n",
        "\n",
        "# Calculate the interquartile range (IQR)\n",
        "iqr = q3 - q1\n",
        "\n",
        "# Define lower and upper bounds for outlier detection\n",
        "lower_bound = q1 - 1.5 * iqr\n",
        "upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "# Filter out rows with values outside the bounds\n",
        "\n",
        "RidesFinal_df = RidesUpdated_df.filter((col(\"Duration\") >= lower_bound) & (col(\"Duration\") <= upper_bound))\n",
        "\n",
        "print(f\"Dataframe without outliers: {RidesFinal_df.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkrHKA9vsA7J"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import Python libs\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# look at values (random sampling)\n",
        "sample_df=RidesFinal_df.sample(0.001) # 0.001 of 0.5M is 500\n",
        "print(\"sample has \",sample_df.count(),\" elements\")\n",
        "plotSample_pdf = sample_df[[\"Bike Id\", \"Duration\"]].toPandas()\n",
        "plotSample_pdf.plot(kind=\"hist\", y=\"Duration\", x=\"Bike Id\") # Hist plot (duration .v. bikeID)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoU-Rg3KsArW"
      },
      "outputs": [],
      "source": [
        "RidesFinal_df.agg({\"Duration\": \"min\"}).show()\n",
        "RidesFinal_df.agg({\"Duration\": \"mean\"}).show()\n",
        "RidesFinal_df.agg({\"Duration\": \"max\"}).show()\n",
        "RidesFinal_df.agg({\"Duration\": \"skewness\"}).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgxLMJ8ssAXe"
      },
      "outputs": [],
      "source": [
        "# so vast majority of rides in this month have \"Duration\" under 2 hours\n",
        "total = RidesFinal_df.count()\n",
        "numExceed = RidesFinal_df.filter(\"Duration > 7200\").count()\n",
        "print(\"Of all rides, percentage over 2 hours:\", (numExceed*100)/total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ2lAVYm6KoI"
      },
      "outputs": [],
      "source": [
        "# TO DO: add a histogram of ride Duration less or equal to 2 hours\n",
        "\n",
        "less_equal2_df = RidesFinal_df.filter(\"Duration <= 7200\")\n",
        "\n",
        "plot_pdf = less_equal2_df[[\"Duration\"]].toPandas() # take just \"Duration\" col, convert to pandas dataframe structure ready to plot\n",
        "#print(\"we have \", plot_df.count(),\" elements to plot\")\n",
        "print(\"we use '_pdf' to denote 'pandas data frame\")\n",
        "\n",
        "plot_pdf.plot(kind=\"hist\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8lgMqHl6u-m"
      },
      "outputs": [],
      "source": [
        "# can refine further\n",
        "# e.g. remove rides with Duration of 0 seconds or over 3600\n",
        "rides_df = RidesFinal_df.filter(\"Duration > 0\").filter(\"Duration <= 3600\")[[\"Duration\"]]\n",
        "# now convert to Pandas df and plot\n",
        "rides_df.toPandas().plot(kind=\"hist\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWYoc9oE7CSS"
      },
      "outputs": [],
      "source": [
        "# TO DO: manually use threshold for given month\n",
        "print(\"histogram with 100 bins and log of frequency for rides lasting more than 5 minutes and less than 10mins\")\n",
        "RidesFinal_df.filter(\"Duration > 300\").filter(\"Duration < 600\")[[\"Duration\"]].toPandas().plot(kind=\"hist\", bins=10, log=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RDlANAuLq9W"
      },
      "outputs": [],
      "source": [
        "# group \"StartStation Id\" and sum their \"Duration\", sort in descending order\n",
        "StartStation_df = (RidesFinal_df.select(\"StartStation Id\", \"StartStation Name\", \"EndStation Id\", \"EndStation Name\",\"Duration\")\n",
        "         .groupBy(\"StartStation Id\",\"StartStation Name\")).agg(sum(\"Duration\").alias(\"Total Duration\")).orderBy(\"Total Duration\", ascending=False)\n",
        "# show top 10\n",
        "StartStation_df.show(10)\n",
        "\n",
        "print('Note \"Duration\" is in seconds (see above table)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hmvh99HaLtLo"
      },
      "outputs": [],
      "source": [
        " # group \"EndStation Id\" and sum their \"Duration\", sort in descending order\n",
        "EndStation_df = RidesFinal_df.select(\"EndStation Id\", \"EndStation Name\", \"Duration\")\\\n",
        ".groupBy(\"EndStation Id\", \"EndStation Name\").agg(sum(\"Duration\").alias(\"Total Duration\")).orderBy(\"Total Duration\", ascending=False)\n",
        "# show top 10\n",
        "EndStation_df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMDZ71FLuJ-_"
      },
      "outputs": [],
      "source": [
        "#determine %age of rides >3 days and interpret what this means\n",
        "\n",
        "\n",
        "# Count the number of rides with duration greater than 3 days\n",
        "Rides_greater_3_days = RidesFinal_df.filter(col(\"Duration\") > (259200))  # Assuming duration is in minutes, convert 3 days to minutes (3 * 24 * 60)\n",
        "\n",
        "# Calculate the total number of rides\n",
        "Total_rides = RidesFinal_df.count()\n",
        "Rides_greater_3_days.count()\n",
        "\n",
        "# Calculate the percentage of rides greater than 3 days\n",
        "Percentage_greater_3_days = (Rides_greater_3_days.count() / Total_rides) * 100\n",
        "\n",
        "print(\"Percentage of rides greater than 3 days:\", Percentage_greater_3_days)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33O4N0wruQjA"
      },
      "outputs": [],
      "source": [
        "#determine how many rows have Duration of over a day\n",
        "# Calculate the number of rows where the duration is over a day\n",
        "rows_over_day = RidesFinal_df.filter(col(\"Duration\") > (24 * 60))  # Assuming duration is in minutes, convert 1 day to minutes (24 * 60)\n",
        "\n",
        "# Count the number of rows\n",
        "count_over_day = rows_over_day.count()\n",
        "\n",
        "print(\"Number of rows with duration over a day:\", count_over_day)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VdyiRb1tUqR"
      },
      "outputs": [],
      "source": [
        "#Determine popular routes by 'mode' (e.g. short, day hire, commuting etc)\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Define rules for determining the mode based on ride duration\n",
        "mode_description = (\n",
        "    when(col(\"Duration\") <= 30, \"short\")  # Rides with duration less than or equal to 30 minutes are considered \"short\"\n",
        "    .when((col(\"Duration\") > 30) & (col(\"Duration\") <= 1440), \"Day Hire\")  # Rides between 30 minutes and 1 day are considered \"day hire\"\n",
        "    .otherwise(\"Commuting\")  # All other rides are considered \"commuting\"\n",
        ")\n",
        "\n",
        "# Create the new \"Mode\" field based on the defined rules\n",
        "RidesFinal_Mode_df = RidesFinal_df.withColumn(\"Mode\", mode_description)\n",
        "\n",
        "# Filter the dataset based on the mode\n",
        "ShortMode_df = RidesFinal_Mode_df.filter(col(\"Mode\") == \"Short\")\n",
        "DayHireMode_df = RidesFinal_Mode_df.filter(col(\"Mode\") == \"Day Hire\")\n",
        "CommutingMode_df = RidesFinal_Mode_df.filter(col(\"Mode\") == \"Commuting\")\n",
        "\n",
        "RidesFinal_Mode_df.show(10)\n",
        "\n",
        "# Group and aggregate data of the modes tondetermine the popular routes\n",
        "ShortPopularRoutes_df = (ShortMode_df\n",
        "                           .groupBy(\"StartStation Id\", \"EndStation Id\")\n",
        "                           .count()\n",
        "                           .orderBy(\"count\", ascending=False))\n",
        "\n",
        "DayHirePopularRoutes_df = (DayHireMode_df\n",
        "                              .groupBy(\"StartStation Id\", \"EndStation Id\")\n",
        "                              .count()\n",
        "                              .orderBy(\"count\", ascending=False))\n",
        "\n",
        "CommutingPopularRoutes_df = (CommutingMode_df\n",
        "                                .groupBy(\"StartStation Id\", \"EndStation Id\")\n",
        "                                .count()\n",
        "                                .orderBy(\"count\", ascending=False))\n",
        "\n",
        "print(f\"Popular Routes for Short Duration: {ShortPopularRoutes_df.count()}\")\n",
        "print(f\"Popular Routes for a Day Hire Duration: {DayHirePopularRoutes_df.count()}\")\n",
        "print(f\"Popular Routes for Communting Duration: {CommutingPopularRoutes_df.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qttOolg_t77u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEmsPNbo869H"
      },
      "outputs": [],
      "source": [
        "# Filter rows where the \"station name\" column equals \"Baylis Road, Waterloo\"\n",
        "Baylis_df = RidesFinal_df.filter(col(\"StartStation Name\") == \"Baylis Road, Waterloo\")\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "Baylis_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmSSHcKv9vbA"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean duration of Baylis Station\n",
        "mean_duration = Baylis_df.agg({\"Duration\": \"mean\"}).collect()[0][0]\n",
        "\n",
        "print(\"Mean Duration for Baylis Road, Waterloo:\", mean_duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMnvVRoH-Mj1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Filter rows where the \"station name\" column is not \"Baylis Road, Waterloo\"\n",
        "other_stations_df = RidesFinal_df.filter(~(col(\"StartStation Name\") == \"Baylis Road, Waterloo\"))\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "other_stations_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahqol1uZ-n4G"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean duration of other stations\n",
        "mean_duration_other_stations = other_stations_df.agg({\"Duration\": \"mean\"}).collect()[0][0]\n",
        "\n",
        "print(\"Mean Duration for Other Stations:\", mean_duration_other_stations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAsV8FDr-xv1"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.sql.functions import month, to_date\n",
        "\n",
        "# Extract month from the \"Start Date\" column for both DataFrames\n",
        "Baylis_month_df = Baylis_df.withColumn(\"Start Date\", to_date(\"Start Date\", \"dd-MM-yyyy mm:ss\"))\\\n",
        "                     .withColumn(\"Month\",month(\"Start Date\"))\n",
        "other_stations_month_df = other_stations_df.withColumn(\"Start Date\", to_date(\"Start Date\", \"dd-MM-yyyy mm:ss\"))\\\n",
        "                     .withColumn(\"Month\",month(\"Start Date\"))\n",
        "\n",
        "# Show the resulting DataFrames\n",
        "Baylis_month_df.show()\n",
        "other_stations_month_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPlspP992Ozs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1HhrlkPJWRH"
      },
      "outputs": [],
      "source": [
        "#from pyspark.sql.functions import month, col, to_date\n",
        "# Extract month from the \"Start Date\" column for both DataFrames\n",
        "Baylis_df = Baylis_df.withColumn(\"Month\", month(\"Start Date\"))\n",
        "Other_stations_df = other_stations_df.withColumn(\"Month\", month(\"Start Date\"))\n",
        "\n",
        "# Group by month and StartStation Name for both DataFrames\n",
        "Baylis_Monthgrouped_df = Baylis_df.groupBy(\"Month\", \"StartStation Name\").count()\n",
        "Other_stations_Monthgrouped_df = Other_stations_df.groupBy(\"Month\", \"StartStation Name\").count()\n",
        "\n",
        "# Show the resulting DataFrames\n",
        "Baylis_Monthgrouped_df.show()\n",
        "Other_stations_Monthgrouped_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHXcF_vYQCVK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4SL-9ZMQDd0"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import mean\n",
        "\n",
        "# Group by month and calculate the mean duration for Baylis station\n",
        "Baylis_mean_duration_df = Baylis_df.groupBy(\"Month\").agg(mean(\"Duration\").alias(\"Baylis Mean Duration\"))\n",
        "\n",
        "# Group by month and calculate the mean duration for other stations\n",
        "Other_stations_mean_duration_df = Other_stations_df.groupBy(\"Month\").agg(mean(\"Duration\").alias(\"Other Stations Mean Duration\"))\n",
        "\n",
        "# Show the resulting DataFrames\n",
        "Baylis_mean_duration_df.show()\n",
        "Other_stations_mean_duration_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kASxTX-aMQLs"
      },
      "outputs": [],
      "source": [
        "# Join the two DataFrames on the \"Month\" column\n",
        "Baylis_Others_df = Baylis_mean_duration_df.join(Other_stations_mean_duration_df, \"Month\", \"outer\")\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "Baylis_Others_df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7R_HDGNNYdu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract data from the joined DataFrame\n",
        "months = Baylis_Others_df.select(\"Month\").collect()\n",
        "baylis_mean_duration = Baylis_Others_df.select(\"Baylis Mean Duration\").collect()\n",
        "other_stations_mean_duration = Baylis_Others_df.select(\"Other Stations Mean Duration\").collect()\n",
        "\n",
        "# Convert the data to lists for plotting\n",
        "months = [row[\"Month\"] for row in months]\n",
        "baylis_mean_duration = [row[\"Baylis Mean Duration\"] for row in baylis_mean_duration]\n",
        "other_stations_mean_duration = [row[\"Other Stations Mean Duration\"] for row in other_stations_mean_duration]\n",
        "\n",
        "# Plot the mean duration for Baylis station\n",
        "plt.plot(months, baylis_mean_duration, label=\"Baylis Road, Waterloo\")\n",
        "\n",
        "# Plot the mean duration for other stations\n",
        "plt.plot(months, other_stations_mean_duration, label=\"Other Stations\")\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Mean Duration\")\n",
        "plt.title(\"Mean Duration Comparison\")\n",
        "plt.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqCpWMbD1aDz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Group by EndStation Name and count the frequency\n",
        "station_frequency = RidesFinal_df.groupBy(\"EndStation Name\").agg(count(\"*\").alias(\"Frequency\")).orderBy(\"Frequency\", ascending=False)\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "station_frequency.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gWOo2zv5FC_"
      },
      "outputs": [],
      "source": [
        "# Plot routes from Baylis to other stations(heatmap for popularity)\n",
        "\n",
        "# Filter rows where the StartStation Name is Baylis\n",
        "baylis_df = RidesFinal_df.filter(col(\"StartStation Name\") == \"Baylis Road, Waterloo\")\n",
        "\n",
        "# Filter the top ten stations\n",
        "top_ten_stations = station_frequency.limit(20)\n",
        "\n",
        "# Join the filtered Baylis DataFrame with the station_frequency DataFrame\n",
        "baylis_with_frequency_df = baylis_df.join(top_ten_stations, \"EndStation Name\", \"inner\")\n",
        "\n",
        "# Aggregate data to count frequency of rides from Baylis to each top station\n",
        "baylis_to_top_stations_freq = baylis_with_frequency_df.groupBy(\"StartStation Name\", \"EndStation Name\").count()\n",
        "\n",
        "# Convert DataFrame to a format suitable for plotting heatmap\n",
        "heatmap_data = baylis_to_top_stations_freq.toPandas()\n",
        "\n",
        "# Plot heatmap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "heatmap = sns.heatmap(heatmap_data.pivot(index=\"StartStation Name\", columns=\"EndStation Name\", values=\"count\"), cmap=\"YlGnBu\", annot=True, fmt=\"g\")\n",
        "heatmap.set_title(\"Popularity Heatmap: Baylis Station to Top 10 Stations\")\n",
        "plt.xlabel(\"End Station\")\n",
        "plt.ylabel(\"Start Station (Baylis)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX9J_7NyXKXR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Convert joined DataFrame to Pandas DataFrame\n",
        "Baylis_Others_df_pandas = Baylis_Others_df.toPandas()\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t_stat, p_value = ttest_ind(Baylis_Others_df_pandas[\"Baylis Mean Duration\"],\n",
        "                            Baylis_Others_df_pandas[\"Other Stations Mean Duration\"],\n",
        "                            alternative='two-sided')\n",
        "\n",
        "# Set significance level\n",
        "alpha = 0.05\n",
        "\n",
        "\n",
        "\n",
        "print(f\"P_value: {p_value}\")\n",
        "\n",
        "# Interpret results\n",
        "if p_value < alpha:\n",
        "    print(\"Reject Null Hypothesis: There is evidence to suggest that the mean duration of rides from Baylis Road is shorter than other stations.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: There is no evidence to suggest a significant difference in mean ride durations between Baylis Road and other stations.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOfCJAP3rfoehZu3I4lccih",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}